{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import Multi30k\n",
    "import spacy\n",
    "from model import Transformer\n",
    "from data_utils import generate_mask_src, generate_mask_trg, make_vocab, Specials, TextPadderDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "SEQ_LEN = 32\n",
    "EPOCHS = 30\n",
    "PAD_TOKEN_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(batch_size=32, seq_len=64, device='cpu'):\n",
    "    train_iter, valid_iter, test_iter = Multi30k('./datasets/Multi30k/', )\n",
    "    \n",
    "    en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "    de_tokenizer = spacy.load('de_core_news_sm')\n",
    "    # Disable all of the unnecessary pipes to accelerate the data pipeline.\n",
    "    en_tokenizer.disable_pipes(['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "    de_tokenizer.disable_pipes(['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
    "    \n",
    "    de_vocab = make_vocab(\n",
    "        text_iter=[trg for trg, src in train_iter], \n",
    "        tokenizer=de_tokenizer, \n",
    "        specials=(Specials.UNK, Specials.PAD, Specials.SOS, Specials.EOS), \n",
    "        min_freq=1,\n",
    "        voc_cache_name='./datasets/Multi30k/de_vocab.pkl'\n",
    "    )\n",
    "    en_vocab = make_vocab(\n",
    "        text_iter=[src for trg, src in train_iter], \n",
    "        tokenizer=en_tokenizer, \n",
    "        min_freq=1, \n",
    "        specials=(Specials.UNK, Specials.PAD), \n",
    "        voc_cache_name='./datasets/Multi30k/en_vocab.pkl'\n",
    "    )\n",
    "    \n",
    "    my_dataset = TextPadderDataset(\n",
    "        src_vocab=en_vocab, trg_vocab=de_vocab,\n",
    "        text_iterator=train_iter,\n",
    "        src_tokenizer_fn=lambda line: [token.text for token in en_tokenizer(line.lower())],\n",
    "        trg_tokenizer_fn=lambda line: [token.text for token in de_tokenizer(line.lower())],\n",
    "        max_seq_length=seq_len,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size)\n",
    "    return loader, valid_iter, en_vocab, de_vocab, en_tokenizer, de_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "dataloader, valid_iter, en_vocab, de_vocab, en_tokenizer, de_tokenizer = create_dataloader(batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n",
    "model = Transformer(seq_len=SEQ_LEN, src_vocab_size=len(en_vocab), trg_vocab_size=len(de_vocab), dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4, betas=[0.9, 0.98], eps=1e-9)\n",
    "loss_obj = torch.nn.NLLLoss(reduction='sum', ignore_index=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [02:31,  3.98it/s, Epoch/Iteration 0/602. Loss: 24.68532371520996] \n",
      "603it [02:15,  4.46it/s, Epoch/Iteration 1/602. Loss: 21.51140594482422] \n",
      "603it [02:04,  4.86it/s, Epoch/Iteration 2/602. Loss: 21.921279907226562]\n",
      "603it [02:02,  4.92it/s, Epoch/Iteration 3/602. Loss: 18.377609252929688]\n",
      "603it [02:06,  4.76it/s, Epoch/Iteration 4/602. Loss: 17.299686431884766]\n",
      "603it [02:08,  4.67it/s, Epoch/Iteration 5/602. Loss: 14.54904842376709] \n",
      "603it [02:01,  4.97it/s, Epoch/Iteration 6/602. Loss: 13.037145614624023]\n",
      "603it [02:02,  4.91it/s, Epoch/Iteration 7/602. Loss: 14.913864135742188]\n",
      "603it [02:03,  4.88it/s, Epoch/Iteration 8/602. Loss: 12.168251037597656]\n",
      "603it [02:06,  4.76it/s, Epoch/Iteration 9/602. Loss: 11.236723899841309]\n",
      "603it [02:05,  4.82it/s, Epoch/Iteration 10/602. Loss: 8.170421600341797] \n",
      "603it [02:09,  4.65it/s, Epoch/Iteration 11/602. Loss: 9.904566764831543] \n",
      "603it [02:07,  4.74it/s, Epoch/Iteration 12/602. Loss: 9.033177375793457] \n",
      "603it [02:07,  4.73it/s, Epoch/Iteration 13/602. Loss: 7.2677412033081055]\n",
      "603it [02:07,  4.75it/s, Epoch/Iteration 14/602. Loss: 7.086853981018066] \n",
      "603it [02:05,  4.80it/s, Epoch/Iteration 15/602. Loss: 8.05820083618164]  \n",
      "603it [02:04,  4.83it/s, Epoch/Iteration 16/602. Loss: 6.723125457763672] \n",
      "603it [02:10,  4.61it/s, Epoch/Iteration 17/602. Loss: 6.218801021575928] \n",
      "603it [02:09,  4.67it/s, Epoch/Iteration 18/602. Loss: 5.869912624359131] \n",
      "603it [02:10,  4.61it/s, Epoch/Iteration 19/602. Loss: 4.283743858337402] \n",
      "603it [02:06,  4.77it/s, Epoch/Iteration 20/602. Loss: 4.246278285980225] \n",
      "603it [02:13,  4.52it/s, Epoch/Iteration 21/602. Loss: 2.8753533363342285]\n",
      "603it [02:14,  4.47it/s, Epoch/Iteration 22/602. Loss: 4.565969467163086] \n",
      "603it [02:12,  4.54it/s, Epoch/Iteration 23/602. Loss: 4.483336448669434] \n",
      "603it [02:06,  4.76it/s, Epoch/Iteration 24/602. Loss: 2.4478750228881836]\n",
      "603it [02:06,  4.75it/s, Epoch/Iteration 25/602. Loss: 3.290219306945801] \n",
      "603it [02:11,  4.58it/s, Epoch/Iteration 26/602. Loss: 1.7630369663238525]\n",
      "603it [02:11,  4.58it/s, Epoch/Iteration 27/602. Loss: 3.380457639694214] \n",
      "603it [02:09,  4.65it/s, Epoch/Iteration 28/602. Loss: 2.292175054550171] \n",
      "603it [02:02,  4.93it/s, Epoch/Iteration 29/602. Loss: 3.37827467918396]  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    iterator = tqdm(enumerate(dataloader))\n",
    "    for i, (trg_seq, src_seq) in iterator:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        trg_input_seq = trg_seq[:, :-1]\n",
    "        trg_label_seq = trg_seq[:, 1:]\n",
    "        trg_input_seq = trg_input_seq.cuda(); src_seq = src_seq.cuda()\n",
    "        trg_label_seq = trg_label_seq.cuda()\n",
    "\n",
    "        src_mask = generate_mask_src(src_seq, PAD_TOKEN_ID)\n",
    "        trg_mask = generate_mask_trg(trg_input_seq, PAD_TOKEN_ID)\n",
    "\n",
    "        enc_out, logits = model(src_seq, trg_input_seq, src_mask, trg_mask)\n",
    "        loss = loss_obj(logits.view(-1, logits.shape[-1]), trg_label_seq.view(-1)) / float(BATCH_SIZE)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        iterator.set_postfix_str(f\"Epoch/Iteration {epoch}/{i}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter_ = iter(valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen',\n",
       " 'A group of men are loading cotton onto a truck')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg, src = next(valid_iter_)\n",
    "trg, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoding import GreedyDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyDecoder(\n",
    "    model=model, \n",
    "    tokenizer=lambda sent: [token.text for token in en_tokenizer(sent)], \n",
    "    src_vocab=en_vocab, trg_vocab=de_vocab, \n",
    "    eos_token_id=3, sos_token_id=2, pad_token_id=PAD_TOKEN_ID, \n",
    "    max_seq_length=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eine gruppe von männern beugt sich nach links eines lkw .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transl = decoder.decode(src)\n",
    "transl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = Multi30k('./datasets/Multi30k/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import calculate_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:29<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 corpus score = 0.2122980863894124, corpus length = 998.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2122980863894124"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu_score(\n",
    "    decoder=decoder, \n",
    "    dataloader=[(trg, src) for trg, src in test], \n",
    "    src_preprocess_fn=lambda x: [token.text for token in en_tokenizer(x.lower())], \n",
    "    trg_preprocess_fn=lambda x: [token.text for token in de_tokenizer(x.lower())], \n",
    "    max_len=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
