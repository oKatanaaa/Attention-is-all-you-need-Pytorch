{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "from data_utils import create_dataloader, generate_mask_src, generate_mask_trg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 64\n",
    "EPOCHS = 5\n",
    "PAD_TOKEN_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the dataset. Initial size: 1000000\n",
      "Removing special characters...\n",
      "Filtered successfully. Final size: 938171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 891262/891262 [01:24<00:00, 10558.77it/s]\n",
      "100%|██████████| 891262/891262 [02:13<00:00, 6695.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader, valid_iter, en_vocab, de_vocab, en_tokenizer, de_tokenizer = create_dataloader(batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n",
    "model = Transformer(seq_len=SEQ_LEN, src_vocab_size=len(de_vocab), trg_vocab_size=len(en_vocab), dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ru_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(de_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4, betas=[0.9, 0.98], eps=1e-9)\n",
    "loss_obj = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=PAD_TOKEN_ID, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    iterator = tqdm(enumerate(dataloader))\n",
    "    for i, (trg_seq, src_seq) in iterator:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        trg_input_seq = trg_seq[:, :-1]\n",
    "        trg_label_seq = trg_seq[:, 1:]\n",
    "        trg_input_seq = trg_input_seq.cuda(); src_seq = src_seq.cuda()\n",
    "        trg_label_seq = trg_label_seq.cuda()\n",
    "\n",
    "        src_mask, n_src_tokens = generate_mask_src(src_seq, PAD_TOKEN_ID)\n",
    "        trg_mask, n_trg_tokens = generate_mask_trg(trg_input_seq, PAD_TOKEN_ID)\n",
    "\n",
    "        enc_out, logits = model(src_seq, trg_input_seq, src_mask, trg_mask)\n",
    "        loss = loss_obj(logits.view(-1, logits.shape[-1]), trg_label_seq.view(-1)) / float(BATCH_SIZE)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        iterator.set_postfix_str(f\"Epoch/Iteration {epoch}/{i}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import YandexDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the dataset. Initial size: 1000000\n",
      "Removing special characters...\n",
      "Filtered successfully. Final size: 938171\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = YandexDataset('data/datasets/Yandex').get_iters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_iter_ = iter(valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('we were even given additional towels and special sheets for our baby.',\n",
       " 'все просьбы к персоналу выполнялись быстро и безотказно.')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg, src = next(train_iter_)\n",
    "trg, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoding import GreedyDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyDecoder(\n",
    "    model, \n",
    "    lambda sent: [token.text for token in de_tokenizer(sent)], \n",
    "    src_vocab=de_vocab, trg_vocab=en_vocab, \n",
    "    eos_token_id=3, sos_token_id=2, pad_token_id=PAD_TOKEN_ID, \n",
    "    max_seq_length=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am an language model , which was inspired by english translation of russian'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.decode('Я языковая модель, которая была обучена переводить русский на английский')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14817"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_vocab['покупает']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_vocab['верну']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'weights/weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/Yandex/weights_filtered.pth', map_location='cuda:0'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'yandex_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_bleu_score(decoder, dataloader, src_preprocess_fn, trg_preprocess_fn, max_len=10):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for trg, src in tqdm(list(dataloader)):\n",
    "        out = decoder.decode(src)\n",
    "        preds.append(trg_preprocess_fn(out))\n",
    "        targets.append([trg_preprocess_fn(trg)])\n",
    "\n",
    "    bleu_score = corpus_bleu(targets, preds)\n",
    "    print(f'BLEU-4 corpus score = {bleu_score}, corpus length = {len(targets)}.')\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:08<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 corpus score = 0.23002674009458673, corpus length = 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23002674009458673"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu_score(decoder, [x for x in test_iter][:1000], lambda x: [token.text for token in de_tokenizer(x)], lambda x: [token.text for token in en_tokenizer(x)], max_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
